"""
Multiple ML Models Training and ONNX Conversion
ì—¬ëŸ¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ë¹„êµ
"""

import numpy as np
import pandas as pd
import json
import shutil
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from xgboost import XGBRegressor
import onnx
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType
import onnxmltools
from onnxmltools.convert import convert_xgboost as convert_xgb_to_onnx
import onnxruntime as rt


# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path("/Users/sangwon/Project/Sesac_class/bluedonulab-01/match_ML")
DATA_DIR = BASE_DIR / "data"
MODELS_DIR = BASE_DIR / "models"
BEST_MODEL_DIR = BASE_DIR / "best_model"


def load_data():
    """í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    df = pd.read_csv(DATA_DIR / "training_data.csv")

    # Feature columns - ê°œì„ ëœ Feature ì‚¬ìš©
    feature_cols = [
        # ê¸°ë³¸ ì°¨ì´ê°’ (ê°€ì¥ ì¤‘ìš”)
        'empathy_diff', 'patience_diff', 'activity_diff', 'independence_diff',
        # í†µê³„ì  Feature
        'max_diff', 'avg_diff',
        # ë¹„ì„ í˜• Feature
        'empathy_diff_sq', 'patience_diff_sq',
        # ìƒí˜¸ì‘ìš© Feature
        'empathy_patience_interaction',
        # ì›ë³¸ ê°’ë„ í¬í•¨ (ì¶”ê°€ ì •ë³´)
        'patient_empathy', 'patient_patience',
        'caregiver_empathy', 'caregiver_patience'
    ]

    X = df[feature_cols].values
    y = df['satisfaction_score'].values

    return train_test_split(X, y, test_size=0.2, random_state=42), feature_cols


def evaluate_model(y_true, y_pred, model_name):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    # 0-100 ë²”ìœ„ì—ì„œì˜ ì •í™•ë„ (Â±5ì  ì´ë‚´ë¥¼ ì •í™•í•˜ë‹¤ê³  íŒë‹¨)
    accuracy_5 = np.mean(np.abs(y_true - y_pred) <= 5) * 100
    accuracy_10 = np.mean(np.abs(y_true - y_pred) <= 10) * 100

    return {
        'model_name': model_name,
        'rmse': round(rmse, 2),
        'mae': round(mae, 2),
        'r2_score': round(r2, 4),
        'accuracy_within_5': round(accuracy_5, 2),
        'accuracy_within_10': round(accuracy_10, 2)
    }


def convert_to_onnx(model, model_name, X_sample, feature_cols):
    """ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    onnx_path = MODELS_DIR / f"{model_name}.onnx"

    try:
        if isinstance(model, XGBRegressor):
            # XGBoost â†’ ONNX (onnxmltools ì‚¬ìš©)
            print(f"  ğŸ”§ XGBoost ì „ìš© ONNX ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")

            # 1. Booster ê°ì²´ ê°€ì ¸ì˜¤ê¸°
            booster = model.get_booster()

            # 2. ì…ë ¥ íƒ€ì… ì •ì˜
            n_features = X_sample.shape[1]
            initial_type = [('float_input', FloatTensorType([None, n_features]))]

            # 3. XGBoost ì „ìš© ë³€í™˜ í•¨ìˆ˜ ì‚¬ìš© (ì˜¬ë°”ë¥¸ API)
            onnx_model = convert_xgb_to_onnx(
                booster,
                name='XGBoostModel',
                initial_types=initial_type,
                target_opset=12
            )

        else:
            # Sklearn ëª¨ë¸ â†’ ONNX
            initial_type = [('float_input', FloatTensorType([None, X_sample.shape[1]]))]
            onnx_model = convert_sklearn(
                model,
                initial_types=initial_type,
                target_opset=12
            )

        # ONNX ëª¨ë¸ ì €ì¥
        onnx.save_model(onnx_model, str(onnx_path))

        # ONNX ëª¨ë¸ ê²€ì¦
        onnx_model_check = onnx.load(str(onnx_path))
        onnx.checker.check_model(onnx_model_check)

        print(f"  âœ… ONNX ë³€í™˜ ì„±ê³µ: {onnx_path.name}")
        return True

    except Exception as e:
        print(f"  âŒ ONNX ë³€í™˜ ì‹¤íŒ¨ ({model_name}): {e}")
        import traceback
        traceback.print_exc()
        return False


def train_all_models():
    """ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
    print("=" * 60)
    print("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 60)

    # ë°ì´í„° ë¡œë“œ
    (X_train, X_test, y_train, y_test), feature_cols = load_data()
    print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"  - í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ")
    print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ")
    print(f"  - Feature ê°œìˆ˜: {X_train.shape[1]}ê°œ")

    # ëª¨ë¸ ì •ì˜ - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„ 
    models = {
        'linear_regression': LinearRegression(),
        'random_forest': RandomForestRegressor(
            n_estimators=200,        # 100 â†’ 200
            max_depth=15,            # 10 â†’ 15
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        ),
        'xgboost': XGBRegressor(
            n_estimators=200,        # 100 â†’ 200
            max_depth=6,             # 5 â†’ 6
            learning_rate=0.05,      # 0.1 â†’ 0.05 (ë” ëŠë¦¬ì§€ë§Œ ì •í™•)
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        ),
        'svm': SVR(
            kernel='rbf',
            C=100,
            gamma='scale',           # 'auto' â†’ 'scale'
            epsilon=0.1
        ),
        'neural_network': MLPRegressor(
            hidden_layer_sizes=(128, 64, 32),  # (64, 32) â†’ (128, 64, 32)
            max_iter=1000,                      # 500 â†’ 1000
            learning_rate_init=0.001,
            random_state=42,
            early_stopping=True
        )
    }

    results = []

    # ê° ëª¨ë¸ í•™ìŠµ
    for model_name, model in models.items():
        print(f"\n{'='*60}")
        print(f"ğŸ”§ {model_name.upper()} í•™ìŠµ ì¤‘...")
        print(f"{'='*60}")

        # í•™ìŠµ
        model.fit(X_train, y_train)

        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)

        # í‰ê°€
        metrics = evaluate_model(y_test, y_pred, model_name)
        results.append(metrics)

        print(f"\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
        print(f"  - RMSE: {metrics['rmse']:.2f}")
        print(f"  - MAE: {metrics['mae']:.2f}")
        print(f"  - RÂ² Score: {metrics['r2_score']:.4f}")
        print(f"  - Â±5ì  ì´ë‚´ ì •í™•ë„: {metrics['accuracy_within_5']:.2f}%")
        print(f"  - Â±10ì  ì´ë‚´ ì •í™•ë„: {metrics['accuracy_within_10']:.2f}%")

        # ONNX ë³€í™˜
        print(f"\nğŸ”„ ONNX ë³€í™˜ ì¤‘...")
        convert_to_onnx(model, model_name, X_train, feature_cols)

    return results, feature_cols


def find_best_model(results):
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ ì • (RÂ² ê¸°ì¤€)"""
    best_model = max(results, key=lambda x: x['r2_score'])
    return best_model


def copy_best_model(best_model_name):
    """ìµœê³  ëª¨ë¸ì„ best_model í´ë”ì— ë³µì‚¬"""
    src = MODELS_DIR / f"{best_model_name}.onnx"
    dst = BEST_MODEL_DIR / "best_model.onnx"

    if not src.exists():
        print(f"\nâš ï¸  ê²½ê³ : {src}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ONNX ë³€í™˜ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    shutil.copy2(src, dst)
    print(f"\nâœ… ìµœê³  ëª¨ë¸ ë³µì‚¬ ì™„ë£Œ:")
    print(f"  - ì›ë³¸: {src}")
    print(f"  - ë³µì‚¬ë³¸: {dst}")


def save_results(results, best_model, feature_cols):
    """ê²°ê³¼ ì €ì¥"""
    # ì „ì²´ ë¹„êµ ê²°ê³¼
    comparison = {
        'models': results,
        'best_model': best_model,
        'feature_columns': feature_cols,
        'timestamp': pd.Timestamp.now().isoformat()
    }

    comparison_path = BASE_DIR / "model_comparison.json"
    with open(comparison_path, 'w', encoding='utf-8') as f:
        json.dump(comparison, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_path}")

    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì •ë³´
    best_info = {
        'model_name': best_model['model_name'],
        'metrics': best_model,
        'feature_columns': feature_cols,
        'onnx_path': 'best_model.onnx',
        'timestamp': pd.Timestamp.now().isoformat()
    }

    best_info_path = BEST_MODEL_DIR / "model_info.json"
    with open(best_info_path, 'w', encoding='utf-8') as f:
        json.dump(best_info, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì •ë³´ ì €ì¥: {best_info_path}")


def print_summary(results, best_model):
    """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ìµœì¢… ëª¨ë¸ ë¹„êµ ê²°ê³¼")
    print("=" * 60)

    # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
    df_results = pd.DataFrame(results)
    df_results = df_results.sort_values('r2_score', ascending=False)

    print("\n" + df_results.to_string(index=False))

    print("\n" + "=" * 60)
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model_name'].upper()}")
    print("=" * 60)
    print(f"  - RÂ² Score: {best_model['r2_score']:.4f}")
    print(f"  - RMSE: {best_model['rmse']:.2f}ì ")
    print(f"  - Â±10ì  ì´ë‚´ ì •í™•ë„: {best_model['accuracy_within_10']:.2f}%")
    print("=" * 60)


if __name__ == "__main__":
    # ëª¨ë“  ëª¨ë¸ í•™ìŠµ
    results, feature_cols = train_all_models()

    # ìµœê³  ëª¨ë¸ ì„ ì •
    best_model = find_best_model(results)

    # ìµœê³  ëª¨ë¸ ë³µì‚¬
    copy_best_model(best_model['model_name'])

    # ê²°ê³¼ ì €ì¥
    save_results(results, best_model, feature_cols)

    # ìš”ì•½ ì¶œë ¥
    print_summary(results, best_model)

    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“ ONNX ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {MODELS_DIR}")
    print(f"ğŸ† ë² ìŠ¤íŠ¸ ëª¨ë¸ ìœ„ì¹˜: {BEST_MODEL_DIR}")
