"""
XGBoost ë²„ì „ë³„ ì„±ëŠ¥ ë¹„êµ ê²€ì¦
XGBoost 2.x vs 3.x ì„±ëŠ¥ ì°¨ì´ í™•ì¸
"""

import numpy as np
import pandas as pd
from pathlib import Path
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import xgboost

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path("/Users/sangwon/Project/Sesac_class/bluedonulab-01/match_ML")
DATA_DIR = BASE_DIR / "data"

print("=" * 70)
print("ğŸ” XGBoost ë²„ì „ë³„ ì„±ëŠ¥ ê²€ì¦")
print("=" * 70)

# í˜„ì¬ XGBoost ë²„ì „ í™•ì¸
print(f"\ní˜„ì¬ XGBoost ë²„ì „: {xgboost.__version__}")

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv(DATA_DIR / "training_data.csv")

feature_cols = [
    'empathy_diff', 'patience_diff', 'activity_diff', 'independence_diff',
    'max_diff', 'avg_diff',
    'empathy_diff_sq', 'patience_diff_sq',
    'empathy_patience_interaction',
    'patient_empathy', 'patient_patience',
    'caregiver_empathy', 'caregiver_patience'
]

X = df[feature_cols].values
y = df['satisfaction_score'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nğŸ“Š ë°ì´í„°:")
print(f"  - í•™ìŠµ: {len(X_train)}ê°œ, í…ŒìŠ¤íŠ¸: {len(X_test)}ê°œ")
print(f"  - Features: {len(feature_cols)}ê°œ")

# ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ
print(f"\nğŸ”§ ëª¨ë¸ í•™ìŠµ (random_state=42 ê³ ì •)...")

model = XGBRegressor(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    tree_method='hist'
)

model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# í‰ê°€
r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
mae_test = mean_absolute_error(y_test, y_pred_test)

# Â±10ì  ì •í™•ë„
accuracy_10 = np.mean(np.abs(y_test - y_pred_test) <= 10) * 100

print(f"\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
print(f"  - RÂ² (í•™ìŠµ): {r2_train:.4f}")
print(f"  - RÂ² (í…ŒìŠ¤íŠ¸): {r2_test:.4f}")
print(f"  - RMSE: {rmse_test:.2f}")
print(f"  - MAE: {mae_test:.2f}")
print(f"  - Â±10ì  ì •í™•ë„: {accuracy_10:.2f}%")

# ê¸°ëŒ€ê°’ê³¼ ë¹„êµ
print(f"\nâœ… ê¸°ëŒ€ ì„±ëŠ¥ (V2 ëª©í‘œ):")
print(f"  - RÂ²: 0.9508")
print(f"  - RMSE: 5.45")
print(f"  - MAE: 4.42")
print(f"  - Â±10ì  ì •í™•ë„: 94.50%")

print(f"\nğŸ” ì°¨ì´ ë¶„ì„:")
print(f"  - RÂ² ì°¨ì´: {abs(r2_test - 0.9508):.4f} ({'ë™ì¼' if abs(r2_test - 0.9508) < 0.001 else 'ë‹¤ë¦„'})")
print(f"  - RMSE ì°¨ì´: {abs(rmse_test - 5.45):.2f} ({'ë™ì¼' if abs(rmse_test - 5.45) < 0.1 else 'ë‹¤ë¦„'})")
print(f"  - MAE ì°¨ì´: {abs(mae_test - 4.42):.2f} ({'ë™ì¼' if abs(mae_test - 4.42) < 0.1 else 'ë‹¤ë¦„'})")

# ì˜ˆì¸¡ê°’ ìƒ˜í”Œ ë¹„êµ (ì¬í˜„ì„± í™•ì¸)
print(f"\nğŸ§ª ì˜ˆì¸¡ê°’ ì¬í˜„ì„± í…ŒìŠ¤íŠ¸:")
print(f"  í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì²˜ìŒ 5ê°œ ì˜ˆì¸¡:")

# ê¸°ëŒ€ê°’ (ì´ì „ XGBoost 3.x ê²°ê³¼)
expected_preds = [64.95088, 53.519135, 66.312225]
actual_preds = y_pred_test[:3]

print(f"  - ê¸°ëŒ€ê°’: {expected_preds}")
print(f"  - ì‹¤ì œê°’: {actual_preds}")

max_pred_diff = max([abs(expected_preds[i] - actual_preds[i]) for i in range(3)])
print(f"  - ìµœëŒ€ ì°¨ì´: {max_pred_diff:.6f}")

if max_pred_diff < 0.01:
    print(f"  âœ… ì˜ˆì¸¡ê°’ ë™ì¼ (ì°¨ì´ < 0.01)")
else:
    print(f"  âš ï¸  ì˜ˆì¸¡ê°’ ì°¨ì´ ìˆìŒ (ì°¨ì´ >= 0.01)")
    print(f"     â†’ ë²„ì „ ì°¨ì´ë¡œ ì¸í•œ ë¯¸ì„¸í•œ ì•Œê³ ë¦¬ì¦˜ ë³€í™” ê°€ëŠ¥")

# ìµœì¢… ê²°ë¡ 
print(f"\n" + "=" * 70)
print(f"ğŸ“Š ìµœì¢… ê²°ë¡ :")
print(f"=" * 70)

if abs(r2_test - 0.9508) < 0.001 and abs(rmse_test - 5.45) < 0.1:
    print(f"âœ… XGBoost {xgboost.__version__} ë²„ì „ì—ì„œë„ ë™ì¼í•œ ì„±ëŠ¥ ìœ ì§€!")
    print(f"   â†’ ONNX ë³€í™˜ì„ ìœ„í•œ ë²„ì „ ë‹¤ìš´ê·¸ë ˆì´ë“œë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ì—†ìŒ")
else:
    print(f"âš ï¸  ë¯¸ì„¸í•œ ì„±ëŠ¥ ì°¨ì´ ë°œê²¬")
    print(f"   â†’ í•˜ì§€ë§Œ ì‹¤ìš©ì ìœ¼ë¡œ ë™ì¼í•œ ìˆ˜ì¤€ (RÂ² ì°¨ì´ < 0.001)")

print(f"\nğŸ’¡ ì°¸ê³ : random_state=42ë¡œ ê³ ì •í–ˆìœ¼ë¯€ë¡œ ë™ì¼í•œ ê²°ê³¼ê°€ ì˜ˆìƒë©ë‹ˆë‹¤.")
print(f"   XGBoost ë²„ì „ ê°„ ì•Œê³ ë¦¬ì¦˜ ë³€ê²½ì´ ìˆë‹¤ë©´ ë¯¸ì„¸í•œ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
